{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# Path to your subfolder containing the data\n",
        "folder_path = '/content/drive/MyDrive/Colab Notebooks/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW7cw9CADWN1",
        "outputId": "07d6056c-e4a6-427a-eff1-9366d4f20b95"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path1 = folder_path + \"cleaned_data.csv\"\n",
        "dataset_path2 = folder_path + \"one_hot_encoding.csv\""
      ],
      "metadata": {
        "id": "n0-wmThsFVHr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXdeDe6jmT92",
        "outputId": "41dae620-aae0-4456-bea6-7a43d28f3770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "16566/16566 [==============================] - 243s 15ms/step - loss: 0.4377 - acc: 0.8867 - val_loss: 0.4011 - val_acc: 0.8982\n",
            "Epoch 2/10\n",
            "16566/16566 [==============================] - 180s 11ms/step - loss: 0.3629 - acc: 0.9063 - val_loss: 0.3697 - val_acc: 0.9055\n",
            "Epoch 3/10\n",
            "16566/16566 [==============================] - 172s 10ms/step - loss: 0.3322 - acc: 0.9135 - val_loss: 0.3653 - val_acc: 0.9074\n",
            "Epoch 4/10\n",
            "16566/16566 [==============================] - 173s 10ms/step - loss: 0.3056 - acc: 0.9205 - val_loss: 0.3510 - val_acc: 0.9138\n",
            "Epoch 5/10\n",
            "16566/16566 [==============================] - 172s 10ms/step - loss: 0.2829 - acc: 0.9265 - val_loss: 0.3414 - val_acc: 0.9167\n",
            "Epoch 6/10\n",
            "16566/16566 [==============================] - 172s 10ms/step - loss: 0.2636 - acc: 0.9308 - val_loss: 0.3416 - val_acc: 0.9190\n",
            "Epoch 7/10\n",
            "16566/16566 [==============================] - 172s 10ms/step - loss: 0.2453 - acc: 0.9349 - val_loss: 0.3400 - val_acc: 0.9212\n",
            "Epoch 8/10\n",
            "16566/16566 [==============================] - 173s 10ms/step - loss: 0.2281 - acc: 0.9388 - val_loss: 0.3432 - val_acc: 0.9213\n",
            "Epoch 9/10\n",
            "16566/16566 [==============================] - 171s 10ms/step - loss: 0.2111 - acc: 0.9423 - val_loss: 0.3612 - val_acc: 0.9208\n",
            "Epoch 10/10\n",
            "16566/16566 [==============================] - 171s 10ms/step - loss: 0.1945 - acc: 0.9463 - val_loss: 0.3644 - val_acc: 0.9238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Memuat data\n",
        "df = pd.read_csv(dataset_path1)\n",
        "one_hot = pd.read_csv(dataset_path2)\n",
        "\n",
        "# Persiapan label untuk pelatihan\n",
        "rating = [0, 4, 6, 8, 10]\n",
        "new_label = ['bad', 'average', 'good', 'favourite']\n",
        "new_label = {i: label for i, label in zip(rating, new_label)}\n",
        "df[\"rating_label\"] = df['vote_average'].map(new_label)\n",
        "\n",
        "# Menghapus entri tanpa label\n",
        "labeled = df.dropna(subset=['rating_label'])\n",
        "merged = pd.merge(labeled, one_hot, on='title')\n",
        "\n",
        "# Mengubah label ke dalam format numerik\n",
        "mapping_label = {'bad': 0, 'average': 1, 'good': 2, 'favourite': 3}\n",
        "merged['rating_label'] = merged['rating_label'].replace(mapping_label)\n",
        "\n",
        "merged.to_csv('merged.csv')\n",
        "\n",
        "# Tokenisasi dan padding\n",
        "overview_data = merged['overview'].values\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(overview_data)\n",
        "overview_sequences = tokenizer.texts_to_sequences(overview_data)\n",
        "max_len = 100\n",
        "overview_sequences = pad_sequences(overview_sequences, maxlen=max_len)\n",
        "\n",
        "# Split data menjadi training dan testing\n",
        "X = overview_sequences\n",
        "y = merged['rating_label']\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.8, random_state=42)\n",
        "\n",
        "# Definisikan model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(max_len,)),\n",
        "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "# Kompilasi model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "# Latih model\n",
        "model.fit(train_X, train_y, epochs=10, batch_size=32, validation_data=(test_X, test_y))\n",
        "\n",
        "# Simpan model\n",
        "model.save('model_overview.h5')\n",
        "\n",
        "# Konversi ke TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Simpan model TensorFlow Lite\n",
        "with open('model_overview.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n"
      ]
    }
  ]
}